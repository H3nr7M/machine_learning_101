{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/H3nr7M/machine_learning_101/blob/main/Machine_learning_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine learning\n",
        "Machine learning is a rapidly growing field that has transformed the way we approach problems in a variety of domains, including finance, healthcare, and technology. At its core, machine learning involves building algorithms that can automatically learn patterns in data and make predictions or decisions based on those patterns.\n",
        "\n",
        "If you're new to machine learning, it can be overwhelming to know where to start. This repository provides a basic introduction to machine learning, we going to focus on supervised learning, which is the most common type of machine learning problem. We will also cover some of the most important concepts in machine learning, and we build a model and do all the process from scratch.\n",
        "\n",
        "So let's get started!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Machine learning (ML) is a type of artificial intelligence (AI), see the image below, that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Classical machine learning is often categorized by how an algorithm learns to become more accurate in its predictions. There are four basic approaches:supervised learning, unsupervised learning, semi-supervised learning and reinforcement learning. \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src='machine.png'>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is an aproach?\n",
        "\n",
        "An approach is a way of doing something. In machine learning, an approach is a way of training a model, see the imagen below. Those types of approaches will be discussed in the following sections."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src='types.png'>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the aproaches of supervised learning and unsupervised learning we have a lot of algorithms that can be used to train a model and each algorithm has its own characteristics and it's better suited for certain types of problems like classification, regression and clustering in the image below we have a brief of the algorithms and uses of each one."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src='supervised.png'>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised learning\n",
        "\n",
        "In this type of machine learning, data scientists (the person in charge) supply algorithms with labeled training data and define the variables they want the algorithm to assess for correlations. Both the input and the output of the algorithm is specified, types of tasks supervised learning can solve and their algorithms.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src='bannana.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After start with our first project we have to undertand some basic concepts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AOmuyjcCsNUE"
      },
      "source": [
        "## Procesing data\n",
        "\n",
        "In the machine learning training process, data processing is a critical step that involves preparing the data for use in the model training. Here are some common tasks involved in data processing and their importance:\n",
        "\n",
        "- Manage missing values: It is common for datasets to have missing values. These missing values can cause errors in the model training process. Therefore, it is important to handle these missing values before proceeding with the model training. There are several techniques to manage missing values, including dropping the rows with missing values or imputing missing values using techniques such as mean or median imputation.\n",
        "\n",
        "- Label encoding: Many machine learning algorithms require the data to be in numeric form. Therefore, categorical variables need to be converted into numeric form. Label encoding is one of the techniques used to perform this conversion. In label encoding, each unique value in a categorical variable is assigned a unique integer value.\n",
        "\n",
        "- Handle imbalanced datasets: Imbalanced datasets occur when the number of instances in one class is significantly larger or smaller than the other classes. Handling imbalanced datasets is critical to ensure that the model is not biased towards the majority class. Techniques such as oversampling, undersampling, and generating synthetic samples can be used to handle imbalanced datasets.\n",
        "\n",
        "- Standardization of the data: Standardization is the process of scaling the data to have zero mean and unit variance. Standardization is important to ensure that the features are on a similar scale and to improve the performance of some machine learning algorithms.\n",
        "\n",
        "- Split our dataset: It is important to split the data into training and test datasets. The training dataset is used to train the model, while the test dataset is used to evaluate the performance of the model. It is important to ensure that the model is not overfitting to the training data and performs well on unseen data.\n",
        "\n",
        "\n",
        "As you can see, data processing is an essential step in the machine learning training process that involves preparing the data for use in the model training. The tasks involved in data processing, such as managing missing values, label encoding, handling imbalanced datasets, standardization, and data splitting, ensure that the model is trained on high-quality data and performs well on unseen data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-0nuW5DW4PA4"
      },
      "source": [
        "### Manage missings values\n",
        "It is common for datasets to have missing values. These missing values can cause errors in the model training process. Therefore, it is important to handle these missing values before proceeding with the model training. There are several techniques to manage missing values, including dropping the rows with missing values or imputing missing values using techniques such as mean or median imputation.\n",
        "\n",
        "In this project we going to use sklearn.datasets, also we going to use pandas and numpy to manage our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HJTkmnLk2JgZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "btO6vkDI2ddL"
      },
      "outputs": [],
      "source": [
        "# loading the dataset in our folder to a Pandas DataFrame\n",
        "dataset = pd.read_csv('Placement_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11iXl-ck2pBF",
        "outputId": "e4979125-e6b8-49a4-b9c6-1a8d69fcc597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sl_no              0\n",
              "gender             0\n",
              "ssc_p              0\n",
              "ssc_b              0\n",
              "hsc_p              0\n",
              "hsc_b              0\n",
              "hsc_s              0\n",
              "degree_p           0\n",
              "degree_t           0\n",
              "workex             0\n",
              "etest_p            0\n",
              "specialisation     0\n",
              "mba_p              0\n",
              "status             0\n",
              "salary            67\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the following command will show the nulls in the dataset, we have nulls values in the salary column\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SkYt8l4ZrmLp"
      },
      "source": [
        "There are several techniques to manage missing values, including dropping the rows with missing values or imputing missing values using techniques such as mean or median imputation. In section below we show 4 ways to  manage missings values be careful and use the best for your project and don't use all of them in the same project.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KJlQbpIs3TIe"
      },
      "outputs": [],
      "source": [
        "#This command will replace the nulls values with the median of the salary column\n",
        "dataset['salary'].fillna(dataset['salary'].median(),inplace=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This command will replace the nulls values with the mean of the salary column\n",
        "dataset['salary'].fillna(dataset['salary'].mean(),inplace=True) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This command will replace the nulls values with the mode of the salary column\n",
        "dataset['salary'].fillna(dataset['salary'].mode(),inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This command will drop the rows with nulls values\n",
        "dataset = dataset.dropna(how='any')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After manage the nulls values we can run the `dataset.isnull().sum()` and see that there are no nulls values in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci6tGM103GSb",
        "outputId": "481c8ac4-7a5e-4a10-e56e-f411ca8e2237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sl_no             0\n",
              "gender            0\n",
              "ssc_p             0\n",
              "ssc_b             0\n",
              "hsc_p             0\n",
              "hsc_b             0\n",
              "hsc_s             0\n",
              "degree_p          0\n",
              "degree_t          0\n",
              "workex            0\n",
              "etest_p           0\n",
              "specialisation    0\n",
              "mba_p             0\n",
              "status            0\n",
              "salary            0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ih2jHST27PuU"
      },
      "source": [
        "## Label Encoding\n",
        "\n",
        "Many machine learning algorithms require the data to be in numeric form. Therefore, categorical variables need to be converted into numeric form. Label encoding is one of the techniques used to perform this conversion. In label encoding, each unique value in a categorical variable is assigned a unique integer value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BpuirYkV4R7_"
      },
      "outputs": [],
      "source": [
        "# loading the data from csv file to pandas dataFrame\n",
        "cancer_data = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B1IPmMP6LAo",
        "outputId": "a0d4bb22-09a2-4c69-bfab-3b1b4415e4f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "B    357\n",
              "M    212\n",
              "Name: diagnosis, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# finding the count of different labels\n",
        "cancer_data['diagnosis'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you see we have 2 categorical variables in our dataset, B for benign and M for Malign, we going to use the `LabelEncoder` from sklearn to convert our categorical variables into numeric form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k7ocNAsA8XLV"
      },
      "outputs": [],
      "source": [
        "# load the Label Encoder function\n",
        "label_encode = LabelEncoder()\n",
        "labels = label_encode.fit_transform(cancer_data.diagnosis)\n",
        "# appending the labels to the DataFrame\n",
        "cancer_data['target'] = labels"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After convert our categorical variables into numeric form we can see that we have 2 variables with 0 and 1 values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PECWJPml5fk4",
        "outputId": "7da7ef3f-d07d-4950-d6ec-4f35fede3202"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    357\n",
              "1    212\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cancer_data['target'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j7CjwQ2vx0si"
      },
      "source": [
        "## Handle inbalanced datasets\n",
        "\n",
        "Imbalanced datasets occur when the number of instances in one class is significantly larger or smaller than the other classes. Handling imbalanced datasets is critical to ensure that the model is not biased towards the majority class. Techniques such as oversampling, undersampling, and generating synthetic samples can be used to handle imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twd1m3w1x0Qb",
        "outputId": "20a094c4-a6bf-4ba4-c4a0-4539adc4dd88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    212\n",
              "1    212\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make a sample of the data with the same proportion of the labels, because the original proportion was 357 benign and 212 malignant\n",
        "\n",
        "fine = cancer_data[cancer_data.target == 0]\n",
        "wrong = cancer_data[cancer_data.target == 1]\n",
        "\n",
        "fine_sample = fine.sample(n=212)\n",
        "\n",
        "new_dataset = pd.concat([fine_sample, wrong], axis = 0)\n",
        "\n",
        "new_dataset['target'].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have a balanced dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P446oTa24M8_"
      },
      "source": [
        "## Standardization of the data\n",
        "\n",
        "Standardization is the process of scaling the data to have zero mean and unit variance. Standardization is important to ensure that the features are on a similar scale and to improve the performance of some machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HFQ38W1Z4XEU"
      },
      "outputs": [],
      "source": [
        "# loading the dataset\n",
        "dataset = sklearn.datasets.load_breast_cancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_D85s6VO4efe"
      },
      "outputs": [],
      "source": [
        "# See the features of the dataset\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0        17.99         10.38          122.80     1001.0          0.11840   \n",
              "1        20.57         17.77          132.90     1326.0          0.08474   \n",
              "2        19.69         21.25          130.00     1203.0          0.10960   \n",
              "3        11.42         20.38           77.58      386.1          0.14250   \n",
              "4        20.29         14.34          135.10     1297.0          0.10030   \n",
              "\n",
              "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0           0.27760          0.3001              0.14710         0.2419   \n",
              "1           0.07864          0.0869              0.07017         0.1812   \n",
              "2           0.15990          0.1974              0.12790         0.2069   \n",
              "3           0.28390          0.2414              0.10520         0.2597   \n",
              "4           0.13280          0.1980              0.10430         0.1809   \n",
              "\n",
              "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
              "0                 0.07871  ...         25.38          17.33           184.60   \n",
              "1                 0.05667  ...         24.99          23.41           158.80   \n",
              "2                 0.05999  ...         23.57          25.53           152.50   \n",
              "3                 0.09744  ...         14.91          26.50            98.87   \n",
              "4                 0.05883  ...         22.54          16.67           152.20   \n",
              "\n",
              "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
              "0      2019.0            0.1622             0.6656           0.7119   \n",
              "1      1956.0            0.1238             0.1866           0.2416   \n",
              "2      1709.0            0.1444             0.4245           0.4504   \n",
              "3       567.7            0.2098             0.8663           0.6869   \n",
              "4      1575.0            0.1374             0.2050           0.4000   \n",
              "\n",
              "   worst concave points  worst symmetry  worst fractal dimension  \n",
              "0                0.2654          0.4601                  0.11890  \n",
              "1                0.1860          0.2750                  0.08902  \n",
              "2                0.2430          0.3613                  0.08758  \n",
              "3                0.2575          0.6638                  0.17300  \n",
              "4                0.1625          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see we have a dataset with different scales for the different variables of our dataset, so we going to use the `StandardScaler` from sklearn to standardize our data and have a dataset with zero mean and unit variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Jyhu47hc4kpC"
      },
      "outputs": [],
      "source": [
        "# Splitting labels and features\n",
        "X = df \n",
        "Y = dataset.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uaNSrglu5ObZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(569, 30) (455, 30) (114, 30)\n"
          ]
        }
      ],
      "source": [
        "# Splitting the dataset into training and testing data in a 20% proportion\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=3)\n",
        "print(X.shape, X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q_jnVIl96LSt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "228.29740508276657\n"
          ]
        }
      ],
      "source": [
        "# This command will show the mean of the dataset\n",
        "print(dataset.data.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Pnd5fxo56LP6"
      },
      "outputs": [],
      "source": [
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_standardized = scaler.transform(X_train)\n",
        "X_test_standardized = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_X3PbySS6LIM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.8654541077212674\n"
          ]
        }
      ],
      "source": [
        "# Show the mean of the standardized data after the standardization\n",
        "print(X_train_standardized.std())\n",
        "print(X_test_standardized.std())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wouUCVoe546e"
      },
      "source": [
        "## Split our dataset\n",
        "\n",
        "It is important to split the data into training and test datasets. The training dataset is used to train the model, while the test dataset is used to evaluate the performance of the model. It is important to ensure that the model is not overfitting to the training data and performs well on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BYG4LJ318qdG"
      },
      "outputs": [],
      "source": [
        "# loading the data from csv file to pandas dataFrame\n",
        "iris_data = pd.read_csv('iris_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vbinqlvK8wyy"
      },
      "outputs": [],
      "source": [
        "# loding the label encoder, this you know by now :)\n",
        "label_encoder_1 = LabelEncoder()\n",
        "iris_labels = label_encoder_1.fit_transform(iris_data.Species)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "78mzNEdx-l2u"
      },
      "outputs": [],
      "source": [
        "# separating the data and labels\n",
        "iris_data1= iris_data.drop(columns = 'Species', axis=1)\n",
        "X=iris_data1.drop(columns='target',axis=1)\n",
        "Y = iris_data['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "3Nh1LOuM_MWa"
      },
      "outputs": [],
      "source": [
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "standardized_data = scaler.transform(X)\n",
        "X = standardized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gEZwbfkc_m8c"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and testing data in a 20% proportion\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ou-xkari_zv7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 5) (120, 5) (30, 5)\n"
          ]
        }
      ],
      "source": [
        "# This command will show the sizes of the training and testing data\n",
        "print(X.shape, X_train.shape, X_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, this repository provides a basic introduction to machine learning, covering the fundamental concepts and techniques that underpin this field. We explored key concepts such as supervised and unsupervised learning.\n",
        "\n",
        "We also delved into the practical side of machine learning, including managing missing values, label encoding, handling imbalanced datasets, standardization, and data splitting. We covered the importance of these techniques in preparing the data for use in the model training, and we demonstrated how to implement these techniques using popular Python libraries such as Pandas, Scikit-learn.\n",
        "\n",
        "Throughout this repository, we worked on a practical example to illustrate the application of these techniques. We used a real-world dataset and implemented these techniques to prepare the data for use in a machine learning model. This practical example provided a hands-on experience and helped to solidify the concepts covered in the repository.\n",
        "\n",
        "You can use this knowledge to start building your own machine learning models and apply them to real-world problems."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMPokizvtpzD439sMZY8N+Z",
      "include_colab_link": true,
      "name": "procesing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "4bc2433493bc9d3fe25868e8ec9f8892b3c1ef74bbe2eae8fdf58b2898f06aa1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
